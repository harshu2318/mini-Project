{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b093c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BHUSHAN\\AppData\\Local\\Temp\\ipykernel_19936\\982234602.py:17: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\all-about-coding\\machine-learning\\.venv\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# IMPORTS & SETUP\n",
    "# ============================\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from skimage.measure import regionprops\n",
    "from skimage.filters import threshold_otsu\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras\n",
    "from tensorflow.python.framework import ops\n",
    "from time import time\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d666720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================\n",
    "# PATHS (change base_path only)\n",
    "# ============================\n",
    "base_path = r\"C:\\Users\\BHUSHAN\\Downloads\\data_sign_project\\extract\"  # parent folder containing all (682, 682_forg, etc.)\n",
    "\n",
    "features_dir = os.path.join(base_path, \"Features\")\n",
    "train_dir = os.path.join(features_dir, \"Training\")\n",
    "test_dir = os.path.join(features_dir, \"Testing\")\n",
    "testfeature_dir = os.path.join(base_path, \"TestFeatures\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(testfeature_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870bad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# IMAGE PROCESSING FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "def rgbgrey(img):\n",
    "    greyimg = np.zeros((img.shape[0], img.shape[1]))\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[row])):\n",
    "            greyimg[row][col] = np.average(img[row][col])\n",
    "    return greyimg\n",
    "\n",
    "\n",
    "def greybin(img):\n",
    "    blur_radius = 0.8\n",
    "    img = ndimage.gaussian_filter(img, blur_radius)\n",
    "    thres = threshold_otsu(img)\n",
    "    binimg = img > thres\n",
    "    return np.logical_not(binimg)\n",
    "\n",
    "\n",
    "def preproc(path, display=False):\n",
    "    img = mpimg.imread(path)\n",
    "    grey = rgbgrey(img)\n",
    "    binimg = greybin(grey)\n",
    "    r, c = np.where(binimg == 1)\n",
    "    signimg = binimg[r.min():r.max(), c.min():c.max()]\n",
    "    if display:\n",
    "        plt.imshow(signimg, cmap='gray')\n",
    "        plt.show()\n",
    "    return signimg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c200cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FEATURE EXTRACTION FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "def Ratio(img):\n",
    "    a = np.sum(img == True)\n",
    "    total = img.shape[0] * img.shape[1]\n",
    "    return a / total\n",
    "\n",
    "\n",
    "def Centroid(img):\n",
    "    coords = np.argwhere(img)\n",
    "    centroid = np.mean(coords, axis=0) / np.array(img.shape)\n",
    "    return centroid[0], centroid[1]\n",
    "\n",
    "\n",
    "def EccentricitySolidity(img):\n",
    "    r = regionprops(img.astype(\"int8\"))\n",
    "    return r[0].eccentricity, r[0].solidity\n",
    "\n",
    "\n",
    "def SkewKurtosis(img):\n",
    "    h, w = img.shape\n",
    "    x = np.arange(w)\n",
    "    y = np.arange(h)\n",
    "    xp = np.sum(img, axis=0)\n",
    "    yp = np.sum(img, axis=1)\n",
    "    cx = np.sum(x * xp) / np.sum(xp)\n",
    "    cy = np.sum(y * yp) / np.sum(yp)\n",
    "    sx = np.sqrt(np.sum((x - cx) ** 2 * xp) / np.sum(img))\n",
    "    sy = np.sqrt(np.sum((y - cy) ** 2 * yp) / np.sum(img))\n",
    "    skewx = np.sum(xp * (x - cx) ** 3) / (np.sum(img) * sx ** 3)\n",
    "    skewy = np.sum(yp * (y - cy) ** 3) / (np.sum(img) * sy ** 3)\n",
    "    kurtx = np.sum(xp * (x - cx) ** 4) / (np.sum(img) * sx ** 4) - 3\n",
    "    kurty = np.sum(yp * (y - cy) ** 4) / (np.sum(img) * sy ** 4) - 3\n",
    "    return (skewx, skewy), (kurtx, kurty)\n",
    "\n",
    "\n",
    "def getCSVFeatures(path):\n",
    "    img = preproc(path, display=False)\n",
    "    ratio = Ratio(img)\n",
    "    centroid = Centroid(img)\n",
    "    ecc, sol = EccentricitySolidity(img)\n",
    "    skew, kurt = SkewKurtosis(img)\n",
    "    return (ratio, centroid[0], centroid[1], ecc, sol, skew[0], skew[1], kurt[0], kurt[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814f4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# FEATURE CSV GENERATION\n",
    "# ============================\n",
    "\n",
    "def makeCSV():\n",
    "    folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f)) and not f.endswith(\"_forg\")]\n",
    "    print(\"Found persons:\", folders)\n",
    "\n",
    "    for person_folder in folders:\n",
    "        person_id = person_folder\n",
    "        genuine_folder = os.path.join(base_path, person_folder)\n",
    "        forged_folder = os.path.join(base_path, person_folder + \"_forg\")\n",
    "\n",
    "        if not os.path.exists(forged_folder):\n",
    "            print(f\"⚠️ Skipping {person_id} (forged folder missing)\")\n",
    "            continue\n",
    "\n",
    "        genuine_images = sorted([os.path.join(genuine_folder, img)\n",
    "                                for img in os.listdir(genuine_folder)\n",
    "                                if img.endswith('.png') or img.endswith('.jpg')])\n",
    "        forged_images = sorted([os.path.join(forged_folder, img)\n",
    "                               for img in os.listdir(forged_folder)\n",
    "                               if img.endswith('.png') or img.endswith('.jpg')])\n",
    "\n",
    "        # Split data (8 train, 3 test)\n",
    "        train_genuine = genuine_images[:8]\n",
    "        test_genuine = genuine_images[8:]\n",
    "        train_forged = forged_images[:8]\n",
    "        test_forged = forged_images[8:]\n",
    "\n",
    "        train_csv = os.path.join(train_dir, f\"training_{person_id}.csv\")\n",
    "        test_csv = os.path.join(test_dir, f\"testing_{person_id}.csv\")\n",
    "\n",
    "        with open(train_csv, 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            for path in train_genuine:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',1\\n')\n",
    "            for path in train_forged:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',0\\n')\n",
    "\n",
    "        with open(test_csv, 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            for path in test_genuine:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',1\\n')\n",
    "            for path in test_forged:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',0\\n')\n",
    "\n",
    "        print(f\"✅ Features saved for person {person_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found persons: ['001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', 'Features', 'TestFeatures']\n",
      "✅ Features saved for person 001\n",
      "✅ Features saved for person 002\n",
      "✅ Features saved for person 003\n",
      "✅ Features saved for person 004\n",
      "✅ Features saved for person 005\n",
      "✅ Features saved for person 006\n",
      "✅ Features saved for person 007\n",
      "✅ Features saved for person 008\n",
      "✅ Features saved for person 009\n",
      "✅ Features saved for person 010\n",
      "✅ Features saved for person 011\n",
      "✅ Features saved for person 012\n",
      "✅ Features saved for person 013\n",
      "✅ Features saved for person 014\n",
      "✅ Features saved for person 015\n",
      "✅ Features saved for person 016\n",
      "✅ Features saved for person 017\n",
      "✅ Features saved for person 018\n",
      "✅ Features saved for person 019\n",
      "✅ Features saved for person 020\n",
      "✅ Features saved for person 021\n",
      "✅ Features saved for person 022\n",
      "✅ Features saved for person 023\n",
      "✅ Features saved for person 024\n",
      "✅ Features saved for person 025\n",
      "✅ Features saved for person 026\n",
      "✅ Features saved for person 027\n",
      "✅ Features saved for person 028\n",
      "✅ Features saved for person 029\n",
      "✅ Features saved for person 030\n",
      "✅ Features saved for person 031\n",
      "✅ Features saved for person 032\n",
      "✅ Features saved for person 033\n",
      "✅ Features saved for person 034\n",
      "✅ Features saved for person 035\n",
      "✅ Features saved for person 036\n",
      "✅ Features saved for person 037\n",
      "✅ Features saved for person 038\n",
      "✅ Features saved for person 039\n",
      "✅ Features saved for person 040\n",
      "✅ Features saved for person 041\n",
      "✅ Features saved for person 042\n"
     ]
    }
   ],
   "source": [
    "makeCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13838ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m n_classes = \u001b[32m2\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# tf Graph input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m X = \u001b[43mtf\u001b[49m.placeholder(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[38;5;28;01mNone\u001b[39;00m, n_input])\n\u001b[32m     32\u001b[39m Y = tf.placeholder(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[38;5;28;01mNone\u001b[39;00m, n_classes])\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Store layers weight & bias\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# MODEL & TRAINING\n",
    "# ============================\n",
    "\n",
    "n_input = 9  # features\n",
    "\n",
    "def readCSV(train_path, test_path, type2=False):\n",
    "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
    "    train_input = np.array(df.values, dtype=np.float32)\n",
    "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
    "    corr_train = keras.utils.to_categorical(np.array(df.values).flatten(), 2)\n",
    "\n",
    "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
    "    test_input = np.array(df.values, dtype=np.float32)\n",
    "\n",
    "    if not type2:\n",
    "        df = pd.read_csv(test_path, usecols=(n_input,))\n",
    "        corr_test = keras.utils.to_categorical(np.array(df.values).flatten(), 2)\n",
    "        return train_input, corr_train, test_input, corr_test\n",
    "    else:\n",
    "        return train_input, corr_train, test_input\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 7\n",
    "n_hidden_2 = 10\n",
    "n_hidden_3 = 30\n",
    "n_classes = 2\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], seed=1)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], seed=2))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], seed=3)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], seed=4))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    layer_1 = tf.tanh(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "logits = multilayer_perceptron(X)\n",
    "loss_op = tf.reduce_mean(tf.squared_difference(logits, Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "pred = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "def evaluate(train_path, test_path, person_id, type2=False):\n",
    "    # If the person's training CSV does not exist, ask for data and auto-train\n",
    "    if not os.path.exists(train_path):\n",
    "        print(f\"⚠️ No training data found for '{person_id}'\")\n",
    "        print(\"Let's create it now.\")\n",
    "\n",
    "        genuine_folder = input(\"Enter full path to genuine signatures folder: \").strip()\n",
    "        forged_folder = input(\"Enter full path to forged signatures folder: \").strip()\n",
    "\n",
    "        if not os.path.exists(genuine_folder) or not os.path.exists(forged_folder):\n",
    "            print(\"❌ One or both folders don't exist. Please check the paths.\")\n",
    "            return None\n",
    "\n",
    "        print(\"⏳ Extracting features and generating training/testing CSVs...\")\n",
    "        genuine_images = sorted([os.path.join(genuine_folder, img)\n",
    "                                 for img in os.listdir(genuine_folder)\n",
    "                                 if img.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        forged_images = sorted([os.path.join(forged_folder, img)\n",
    "                                for img in os.listdir(forged_folder)\n",
    "                                if img.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "        if len(genuine_images) < 5 or len(forged_images) < 5:\n",
    "            print(\"⚠️ Please provide at least 5 genuine and 5 forged signatures for training.\")\n",
    "            return None\n",
    "\n",
    "        # Split data (8 train, 3 test — or all if fewer)\n",
    "        train_genuine = genuine_images[:min(8, len(genuine_images))]\n",
    "        train_forged = forged_images[:min(8, len(forged_images))]\n",
    "        test_genuine = genuine_images[min(8, len(genuine_images)):]\n",
    "        test_forged = forged_images[min(8, len(forged_images)):]\n",
    "\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "        train_csv = os.path.join(train_dir, f\"training_{person_id}.csv\")\n",
    "        test_csv = os.path.join(test_dir, f\"testing_{person_id}.csv\")\n",
    "\n",
    "        with open(train_csv, 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            for path in train_genuine:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',1\\n')\n",
    "            for path in train_forged:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',0\\n')\n",
    "\n",
    "        with open(test_csv, 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "            for path in test_genuine:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',1\\n')\n",
    "            for path in test_forged:\n",
    "                features = getCSVFeatures(path)\n",
    "                handle.write(','.join(map(str, features)) + ',0\\n')\n",
    "\n",
    "        print(f\"✅ New dataset created for '{person_id}'!\")\n",
    "        train_path = train_csv\n",
    "        print(\"✅ Proceeding to training and evaluation...\\n\")\n",
    "\n",
    "    # Normal training/testing process\n",
    "    if not type2:\n",
    "        train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path)\n",
    "    else:\n",
    "        train_input, corr_train, test_input = readCSV(train_path, test_path, type2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(1000):\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
    "            if cost < 0.0001:\n",
    "                break\n",
    "\n",
    "        if not type2:\n",
    "            acc_train = accuracy.eval({X: train_input, Y: corr_train})\n",
    "            acc_test = accuracy.eval({X: test_input, Y: corr_test})\n",
    "            print(f\"✅ Training Accuracy: {acc_train:.4f}, Test Accuracy: {acc_test:.4f}\")\n",
    "            return acc_train, acc_test\n",
    "        else:\n",
    "            prediction = pred.eval({X: test_input})\n",
    "            if prediction[0][1] > prediction[0][0]:\n",
    "                print(\"✅this signature is genuine/ original\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌this signature is forged/ fake \")\n",
    "                return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# TESTING ON A SAMPLE IMAGE\n",
    "# ============================\n",
    "\n",
    "def testing(path):\n",
    "    feature = getCSVFeatures(path)\n",
    "    test_csv_path = os.path.join(testfeature_dir, 'testcsv.csv')\n",
    "    with open(test_csv_path, 'w') as handle:\n",
    "        handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
    "        handle.write(','.join(map(str, feature)) + '\\n')\n",
    "    return test_csv_path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63649e60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m train_person_id = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnter person ID (e.g., 682): \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m test_image_path = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnter full path of signature image to test: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_path = \u001b[43mos\u001b[49m.path.join(train_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_person_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m test_path = testing(test_image_path)\n\u001b[32m      8\u001b[39m evaluate(train_path, test_path, train_person_id, type2=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# === RUNNING TEST ===\n",
    "train_person_id = input(\"Enter person ID (e.g., 682): \")\n",
    "test_image_path = input(\"Enter full path of signature image to test: \")\n",
    "\n",
    "train_path = os.path.join(train_dir, f\"training_{train_person_id}.csv\")\n",
    "test_path = testing(test_image_path)\n",
    "\n",
    "evaluate(train_path, test_path, train_person_id, type2=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
